{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.parse import quote \n",
    "import re\n",
    "\n",
    "hira=['い','う','え','お','は','ひ','ふ','へ','ほ',\n",
    "'か','き','く','け','こ','ま','み','む','め','も',\n",
    "'さ','し','す','せ','そ','や','ゆ','よ',\n",
    "'た','ち','つ','て','と','ら','り','る','れ','ろ',\n",
    "'な','に','ぬ','ね','の','わ','を','ん']\n",
    "pre_url=\"https://ja.wikipedia.org/w/index.php?title=\"\n",
    "pp_pre_url=\"https://ja.wikipedia.org\"\n",
    "\n",
    "def get_html_from_url(fromurl):\n",
    "    html = urlopen(fromurl)\n",
    "    bs_object = BeautifulSoup(html, \"html.parser\")\n",
    "    return bs_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get name list and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get name list\n",
    "category_query=\"%E4%B8%89%E5%9B%BD%E5%BF%97%E3%81%AE%E7%99%BB%E5%A0%B4%E4%BA%BA%E7%89%A9\"\n",
    "\n",
    "href_df=pd.DataFrame(columns=[\"href\", \"title\"])\n",
    "#from each hiragana name index\n",
    "for i in hira:\n",
    "    bsObject=get_html_from_url( pre_url+\"Category:\"+category_query+\"&from=\"+quote(i) ) \n",
    "    tags = bsObject.find_all('div', attrs={'class': 'mw-category-group'})\n",
    "                     \n",
    "    for j in range(0,len(tags)):\n",
    "        ahreflist=tags[j].findAll(href=True)\n",
    "        for k in range(0,len(ahreflist)):\n",
    "            href_df=href_df.append(pd.DataFrame(columns=[\"href\", \"title\"],\n",
    "                data=[[ahreflist[k].attrs[\"href\"],ahreflist[k].attrs[\"title\"]]]))\n",
    "#drop duplication\n",
    "href_df=href_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get people page (not category)\n",
    "peeps_url=href_df[~href_df[\"title\"].str.contains(\"Category\")]\n",
    "\n",
    "peeps_url[\"href\"]=pp_pre_url+peeps_url[\"href\"]\n",
    "peeps_url=peeps_url.reset_index(drop=True)\n",
    "\n",
    "#add columns and initialize \n",
    "peeps_url[\"content\"]=peeps_url[\"category\"]=peeps_url[\"menu\"]=peeps_url[\"table\"]=peeps_url[\"table_lst\"]=''\n",
    "\n",
    "ind=0\n",
    "for i in peeps_url[\"href\"]: \n",
    "    bsObject2=get_html_from_url( i ) \n",
    "    \n",
    "    whole=bsObject2.find_all('div',attrs={'class': 'mw-parser-output'})[0].text\n",
    "    \n",
    "    #append the category elements like sub-list\n",
    "    categories=bsObject2.find_all('div',attrs={'class': 'mw-normal-catlinks'})[0].find_all(\"a\")\n",
    "    category=[]\n",
    "    for j in range(0,len(categories)):\n",
    "        category.append(categories[j].text)\n",
    "    \n",
    "    #contents index of wiki page\n",
    "    if len(bsObject2.find_all('div', attrs={'class': 'toc'}) )>0:\n",
    "        menu=bsObject2.find_all('div', attrs={'class': 'toc'})[0].text\n",
    "    else:\n",
    "        menu=''\n",
    "    \n",
    "    #characteristic table under the page. wiki page's category(hyperlinks)\n",
    "    if len(bsObject2.find_all('table', attrs={'class': 'infobox'}) )>0:\n",
    "        tables=bsObject2.find_all('table', attrs={'class': 'infobox'})[0].text\n",
    "        table_contents=bsObject2.find_all('table',\n",
    "                                          attrs={'class': 'infobox'})[0].find_all(\"tr\")\n",
    "    else:\n",
    "        tables=table_contents=''\n",
    "    table_list=[]\n",
    "    for j in range(0,len(table_contents)): \n",
    "        table_txt=table_contents[j].text\n",
    "        if table_txt.startswith(\"\\n\"):\n",
    "            continue\n",
    "        elif \"\\n\" in table_txt:\n",
    "            table_list.append(table_txt.split(\"\\n\"))\n",
    "            \n",
    "    peeps_url.loc[ind,\"content\"]= whole\n",
    "    peeps_url.loc[ind,\"category\"]= category\n",
    "    peeps_url.loc[ind,\"menu\"]= menu\n",
    "    peeps_url.loc[ind,\"table\"]= tables\n",
    "    peeps_url.loc[ind,\"table_lst\"]= table_list\n",
    "    \n",
    "    ind=ind+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in name(title), remove after (\n",
    "peeps_url[\"title\"]=peeps_url[\"title\"].str.replace(\"\\(.*\", \"\") \n",
    "\n",
    "peeps_url=peeps_url[peeps_url[\"content\"].str.len()>0]\n",
    "\n",
    "#remove unnecessary phrases\n",
    "peeps_url[\"content\"]=peeps_url[\"content\"].apply(lambda x: re.sub(r'参考文献\\[編集\\][\\s\\S]*',\"\",x))\n",
    "peeps_url[\"content\"]=peeps_url[\"content\"].apply(lambda x: re.sub(r'出典\\[編集\\][\\s\\S]*',\"\",x))\n",
    "peeps_url[\"content\"]=peeps_url[\"content\"].apply(lambda x: re.sub(r'この項目は、中国の歴史に関連した書き[\\s\\S]*',\"\",x))\n",
    "peeps_url[\"content\"]=peeps_url[\"content\"].apply(lambda x: re.sub(r'参考資料\\[編集\\][\\s\\S]*',\"\",x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to create pivot. weightdf's index by title column\n",
    "weightdf=peeps_url[[\"title\"]]\n",
    "\n",
    "#create pivot by in-contents frequency\n",
    "def get_weight(nm):\n",
    "    global weightdf\n",
    "    weightdf[nm]=peeps_url[\"content\"].str.count(nm) \n",
    "peeps_url[\"title\"].apply(lambda x: get_weight(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot to long \n",
    "weightdf.index=weightdf[\"title\"]\n",
    "weightdf=weightdf.drop(\"title\",axis=1)\n",
    "weightdf=weightdf.stack().reset_index()\n",
    "#long df's column name name1(source) name2(target) weight of edge\n",
    "weightdf.columns=[\"name1\", \"name2\", \"weight\"]\n",
    "\n",
    "#prep\n",
    "weightdf=weightdf[weightdf[\"name1\"]!=weightdf[\"name2\"]]\n",
    "weightdf=weightdf[weightdf[\"weight\"]>0].reset_index(drop=True)\n",
    "\n",
    "weightdf=weightdf.groupby([\"name1\",\"name2\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create node size(cnt) df\n",
    "node_cnt_df=weightdf[\"name1\"].append(weightdf[\"name2\"]).value_counts().reset_index()\n",
    "node_cnt_df.columns=[\"node_name\", \"cnt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import nxviz as nxv\n",
    "\n",
    "\n",
    "# create the networkx object to get coordinate position\n",
    "G = nx.from_pandas_edgelist(weightdf,  \n",
    "                            source='name1', target='name2' ,edge_attr=\"weight\") \n",
    "\n",
    "pos = nx.spring_layout(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_and_edge(interaction_df, node_coord, node_cnt_df):\n",
    "    node_edge_df1=interaction_df.merge(node_coord.reset_index(), left_on=\"name1\", right_on=\"index\", how=\"inner\")\n",
    "    node_edge_df2=interaction_df.merge(node_coord.reset_index(), left_on=\"name2\", right_on=\"index\", how=\"inner\")\n",
    "\n",
    "    node_edge_merge=node_edge_df1.append(node_edge_df2) \n",
    "    \n",
    "    node_edge_merge2=node_edge_merge.merge(node_cnt_df, left_on=\"name1\", right_on=\"node_name\", how=\"inner\")\n",
    "    \n",
    "    node_edge_merge2=node_edge_merge2.drop(\"index\", axis=1)\n",
    "    node_edge_merge2=node_edge_merge2.drop(\"node_name\", axis=1)\n",
    "    node_edge_merge2=node_edge_merge2.rename(columns={0:\"coord_x\", 1:\"coord_y\", \"cnt\":\"name1_cnt\"})\n",
    "    \n",
    "    #edge name key. source_target \n",
    "    node_edge_df[\"edge_name\"]=node_edge_df[\"name1\"]+\"_\"+node_edge_df[\"name2\"]\n",
    "    node_ege_df=node_ege_df.drop_duplicates(\"edge_name\")\n",
    "    \n",
    "    return node_ege_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_edge_df=node_and_edge(weightdf, pd.DataFrame(pos).transpose(), node_cnt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ege_df.to_csv(\"node_and_edge.csv\", encoding=\"utf-8\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization in\n",
    "### https://public.tableau.com/profile/yj.choi#!/vizhome/easternHaneranetworkplot/1_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
